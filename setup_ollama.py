#!/usr/bin/env python3
"""
Script de configuration et v√©rification d'Ollama pour l'application Django
"""

import requests
import subprocess
import sys
import time
from pathlib import Path

def check_ollama_running():
    """V√©rifie si Ollama est en cours d'ex√©cution"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        return response.status_code == 200
    except:
        return False

def get_available_models():
    """R√©cup√®re les mod√®les disponibles"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            data = response.json()
            return [model['name'] for model in data.get('models', [])]
        return []
    except:
        return []

def pull_model(model_name):
    """T√©l√©charge un mod√®le Ollama"""
    print(f"T√©l√©chargement du mod√®le {model_name}...")
    try:
        result = subprocess.run(
            ["ollama", "pull", model_name], 
            capture_output=True, 
            text=True, 
            check=True
        )
        print(f"‚úÖ Mod√®le {model_name} t√©l√©charg√© avec succ√®s")
        return True
    except subprocess.CalledProcessError as e:
        print(f"‚ùå Erreur lors du t√©l√©chargement : {e}")
        return False
    except FileNotFoundError:
        print("‚ùå Ollama n'est pas install√© ou n'est pas dans le PATH")
        return False

def create_directories():
    """Cr√©e les dossiers n√©cessaires"""
    logs_dir = Path("logs")
    logs_dir.mkdir(exist_ok=True)
    print("‚úÖ Dossier logs cr√©√©")

def main():
    print("üîß Configuration d'Ollama pour l'application Django")
    print("=" * 50)
    
    # Cr√©er les dossiers n√©cessaires
    create_directories()
    
    # V√©rifier si Ollama est en cours d'ex√©cution
    print("1. V√©rification du service Ollama...")
    if not check_ollama_running():
        print("‚ùå Ollama n'est pas en cours d'ex√©cution")
        print("   D√©marrez Ollama avec : ollama serve")
        print("   Ou installez Ollama depuis : https://ollama.ai")
        return False
    
    print("‚úÖ Ollama est en cours d'ex√©cution")
    
    # V√©rifier les mod√®les disponibles
    print("\n2. V√©rification des mod√®les disponibles...")
    models = get_available_models()
    
    if models:
        print(f"‚úÖ Mod√®les disponibles : {', '.join(models)}")
        
        # V√©rifier si llama3.1 est disponible
        preferred_models = ['llama3.1:latest', 'llama3.1', 'llama3:latest', 'llama3']
        has_preferred = any(model in models for model in preferred_models)
        
        if not has_preferred:
            print("‚ö†Ô∏è  Aucun mod√®le LLaMA d√©tect√©")
            response = input("Voulez-vous t√©l√©charger llama3.1? (o/N): ")
            if response.lower() in ['o', 'oui', 'y', 'yes']:
                if pull_model("llama3.1"):
                    print("‚úÖ Mod√®le llama3.1 install√©")
                else:
                    print("‚ùå √âchec de l'installation du mod√®le")
                    return False
        else:
            print("‚úÖ Mod√®le LLaMA disponible")
    else:
        print("‚ùå Aucun mod√®le disponible")
        response = input("Voulez-vous t√©l√©charger llama3.1? (o/N): ")
        if response.lower() in ['o', 'oui', 'y', 'yes']:
            if pull_model("llama3.1"):
                print("‚úÖ Mod√®le llama3.1 install√©")
            else:
                print("‚ùå √âchec de l'installation du mod√®le")
                return False
    
    # Test de connexion
    print("\n3. Test de l'API Ollama...")
    try:
        import ollama
        client = ollama.Client(host='http://localhost:11434')
        
        # Test avec le premier mod√®le disponible
        final_models = get_available_models()
        if final_models:
            test_model = final_models[0]
            print(f"Test avec le mod√®le : {test_model}")
            
            response = client.chat(
                model=test_model,
                messages=[{'role': 'user', 'content': 'Dis juste "OK" en fran√ßais'}],
                options={'num_predict': 10}
            )
            
            print(f"‚úÖ R√©ponse du mod√®le : {response['message']['content']}")
            print("‚úÖ Configuration termin√©e avec succ√®s!")
            return True
        else:
            print("‚ùå Aucun mod√®le disponible pour le test")
            return False
            
    except Exception as e:
        print(f"‚ùå Erreur lors du test : {e}")
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print("\nüéâ Votre application est pr√™te √† utiliser l'IA!")
        print("Vous pouvez maintenant lancer : python manage.py runserver")
    else:
        print("\nüí• Configuration √©chou√©e. Consultez les messages d'erreur ci-dessus.")
        sys.exit(1)